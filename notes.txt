2022/4/13 {
    一阶 markov 有问题吗？
        z 里放 v
        RNN(z, h) 替换成 NN(z)
    东西全放到 h 里了怎么办？
        z 上加变换只是在和 encoder decoder 玩， 与 RNN 无关怎么办
        不可能。 RNN 的输入是 z. 
    双体
        只有二维椭圆轨道哦， ok? 
            ok
        渲染？ 更像 Junyan 还是更像 Xuanjie?
            xuanjie
        背景为黑
            doesn't matter
    读 S3 doc
        最小作用量 = 变分模拟
            和 GAN 有啥关系？
            这里， 物理规律的时候， EBM 的输入是时序。 
            GAN 没有时序， 但也是一种 EBM. 
        如果把对称性理解成 "minimize z capacity", 
            可以把 VAE 换成 AE, 然后加对称性。
        "用对称性去构建context dependency"
            我们做的对称性 focus on 时序规律
}
4/17 {
    evaluate disentanglement
        run linear regression z -> ball_pos
        look at r-squared
}
4/20 {
    two body:
        两个球半径一样？
            不一样？ encoder 无法区分半径和远近。
            let's make them the same. 
        z
            x1, y1, z1, x2, y2, z2
            the color needs to be different, otherwise encoder doesn't know which is which
        实验结果
            模拟
            预测
            解耦
    JEPA slides
}
4/22 {
    pretrain 单球， finetune 双体， 能否解耦双球坐标？
}
4/27 {
    糟糕，双体 DoF 只有 3, 因为质心固定。
        vary 质心？
        6 -> 3? 
        Gus: 6 -> 3
    极坐标不符合对称性规约?
        仅当起抛点固定时， 符合
    RNN 的弱性
        需要
    两点目标： 2d
        datapoints 的 z 受对称性规约
        generalization (interpolation)
            如果不行， 那就是得每个位置的球都见过？
        try cycle
}
4/29 {
    loss 不应该除以 T 和 R
    RNN 预测应该跳过前三帧
}
5/4{
    cycle consistency:
        decoder 不变， encoder 变差
    slides/2d_rnn
        其中：
        有了 recon 也不一定就 不 collapse. 
            估计是为什么 xuanjie 在图片上作 loss 而不是 z 上。
    continue pyxuanjie on hpc, disentangle
        怎么和周五说的不一样? (周五说让我自己设计训练、重现)
    ask yann / alf / cho
        for RNN, GRU, 
            besides hidden_dim, any other way to control its capacity? 
        what do you think of VRNN
}
next {
    VAE 需要数据多点
        我不确定这样做会不会有用
        测试 data efficiency 时， 能不能： 总图片数不变， sequence 数减少？
        用不成视频的图片训练 vae. 
    y is completely noy regularized! 
        many times, x and z are good, but y is piece-wise, and correlated with x, y. 
        VAE+RNN without symmetry can also provide piece-wise, almost-linear z space. 
    how to limit RNN/GRU?
        https://campuswire.com/c/GCEF8E4E7/feed/312
    pitch:
        one-hot vs one-dim
        both can translate!
    AE + symm 几乎总是会崩。 
        decoder gives rubbish when traversing z space. 
        但是 reconstruction 和预测都是好的。
        说明 overfit 非常严重， z space 只有寥寥几个点是正常的小球图片。
    为何 altitude ~ 0 的时候图片是糊的？ 未解之谜。
        it is very consistent. 
    看实验
        epoch_206000
}
6/1 {
    vvrnn
    vvrnn_static
    rnn_min_context
    z_pred_loss_coef
    试一试 平移 和 旋转 分开加
        T R TR I
    grad clip
        xuanjie: 
            1. early-stop before grad explosion
            2. diminishing lr
}
6/15 {
    note to self: if accidentally getting a fast GPU make you waste res, specify early stopping in code. 
}
6/16 {
    plot 1000-epoch loss average
}
6/18 {
    2080Ti: good to go. 
    According to loss plots, overfit happens around 2h of training. 
    神奇！ TR 就会预测不行， T+R 就会更好。
}
6/23 {
    stochastic TRI
        previously: every batch does a number of T+R+TR+I. Diff experiment groups will have diff # of trajectories because of this. 
        Now: stochastically sample one tranformation from the TRI spectification. # of trajs is thus controlled across groups. 
    TRI_5
        the loss still jumps up from time to time! Why? already grad clip. 
}
6/28 {
    grad_clip
        0.02 looks good. 
        Note, this conclusion will change if we change the loss to BCE
        okay, let's do this again. lower grad clip gives faster overfitting???
        Note: grad_clip=1 explosion at EPOCH 72053. 
}
6/29 {
    It seems the consensus is low LR (in my case, low grad clip) leads to higher risk of overfitting. 
        Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates (Smith & Topin 2018)
        My take: Higher stochasticity may help jump out of high local minima. 
    meeting, discuss with Xuanjie
        I:T = 1:13 变成 0:13
        Cycle consistency doesn't help?
}
6/30 {
    grad_clip_3
        grad_clip = 0.03 seems good. 
        0.01: too much overfit
        0.1: sometimes jumps. However, does not look like resets the training progress. 
}
7/6 {
    trying to also print training set pred images. 
        I'm facing overfit. 
        If training pred looks good, then I should make RNN smaller, to avoid overfitting. 
        If training pred looks bad, then RNN should be more powerful. 
    reverb 导致不加对称性也能一维音高
}
7/8 {
    TRI_8
        Training image pred indeed looks much better than validation. Training image pred looks like they succesfully predict. 
        This is surprising. Randomly augmented training set is fit, but validation set is not. Something about augmenting known paths is still weak such that the NNs can overfit. 
        Now that we know it's overfitting, decrease rnn strenth? vae strength?
        trying increasing beta. 
}
7/12 {
    beta_2
        results are weird, but according to vali loss, 1e-5 seems fine. 
}
7/13 {
    vae_complexity_2
        No significant difference! 
            Maybe, VAE size doesn't matter. 
            Maybe, we haven't sufficiently weakened VAE. 
            But! Junyan said sometimes bigger networks will overfit later (虽然久了之后会更 worse)
            Conclusion: let's leave VAE for now, and cripple RNN. Observe whether image pred overfit changes. 
}
7/14 {
    Junyan was right. bigger network -> later overfit -> best epoch has lower validation loss than smaller netowkrs. 
    However, validaiton minimum-loss epoch is not the best recon epoch. later epochs are better, when using evalDecoder. 
        We need a repr metric to tune hyperparameters. 
    Right now, I want to make gigantice networks and predict images well for once. 
}
7/24 {
    dali_3
        Bigger networks are sometimes harder to train. Junyan: "梯度性质不好。" So there's a sweet spot of size. 
}
7/28 {
    结论： RNN 好难驯， 我训不出
    正在问 xuanjie 要最新版， 准备直接 hpc 复现。
}
8 {
    HPC 复现 xuanjie 代码成功。
        可能的我做不成的原因
            没加 teacher forcing
            MSE != BCE
}
8/10 {
    plot z traj for given seq. 
        see encoder -> z
        see rnn -> z
    RTI = 001, see if rnn predicts clearly. 
        maybe my aug is too much. I assumed KLD to work. 
    接下来， 做 two body
        新 idea: 一体 pretrain, 双体 finetune 的时候， 可以多加冗余维度。 可能既学到另一球位置， 又学到质心位置。 
}
8/15 {
    Trying changing vae decoder last layer from tanh to sigmoid. 
        reason: BCE requires prediction to be \in (0, 1)
        Result: 还是老样子
}
8/17 {
    BCE vs MSE
        没啥区别。。。
        然后意识到
            BCE 可能好处是预测准的时候更像 L1
                所以可能可以用 L1 替代
            BCE 在预测正确的情况下也不是 0, 只要 ground truth 不是黑或白。
    plot z traj for given seq. (see 8/10)
        only plotted vae encoded, and not rnn (yet)
        乱线。 as expected
}
8/24 {
    Trying to use teacher forcing, T:R:I=0:0:1, big RNN size. 
}
9/7 {
    Teacher forcing exps
        result: no use
}
9/14 {
    trying to de-emphasize the recon loss, to see if rnn pred is good. 
        0.1:1 recon good pred bad
}
9/16 {
    loss weight exp conclusion:
        lowering self-recon loss weight increases pred loss. 
    next, trying to lower pred loss weight instead. 
}
9/17 {
    lowering pred loss by 0.3 and 0.1 does not affect pred loss. There's even a minor increase. 
        try again, with more rand_inits. 
}
9/18 {
    loss weight:
        pred loss ~= 0.5 seems to provide low losses, but none of the exps successfully predicts the video. 
    next step:
        compare loss=1 with loss=0.5 to gain statistical significance. 
        Meanwhile, develop supervised RNN training. 
}
9/19 {
    z visualization should use 512 dataset
}
9/20 {
    loss weight:
        not siginificant. We will use 1:1. 
}
9/27 {
    // fig 1 smaller. Too big now!!
}
9/28 {
    result/supervise
        maybe Bug? validate_img_prediction loss shouldn't be zero!
}
10/17 {
    Debug success! SUpervised training works. 
    
    Remove supervision. 
    
    teacher forcing for 1e+5 epochs looks good. 
    
    exp: symmetry training. 
}
10/19 {
    testing rnn width, teacher forcing duration, and z pred loss. 
    
    I'm trying to make z truly Gaussian. 
        One challenge is with rejection sampling. 
        Xuanjie: reject if the ball leaves the view. 
        This kind of "clear cut" creates edges in the distribution, which deviates from gaussian. 
        Why not let rnn predict empty frames? 
        What if SPS makes RNN predict correct z even when the ball is out of view? 
        On a tangent: object permanence. Occlude an object and check z. 
        This disallows JEPA. 
        This is a problem of JEPA! 
        Make slides! 
        对称性应加在 hidden state 和 embedding 之间的 bottleneck 上。
    
    teacher_forcing_duration_no_z_loss_2022_Oct_19_15;40;28
        See plot.pdf
        - once TF ends, or, once TF approaches 0 and slows down, overfitting begins. 
        - duration = 30k is long enough. we may even accelerate. 
        - According to linear proj mse during training, disentanglement does require long-range RNN prediction, i.e. it only gets good when TF diminishes. 
    
    two body: dataset
        analyzed distribution. run `physics_two_body.py`
        
        preview. set OBV_ONLY and run `render_dataset_two_body.py`
            most don't orbit. This is to prevent correlation between momentum! 
            i.e., enforcing orbiting will make the task harder. But maybe SPS will manage to disentangle anyways! 
    
    what are some symmetry assumptions?
        method 1: the theoretic perspective. study group theory
        method 2: the engineering perspective. treat (trans, untrans) as matrix operations. Constrained operations correspond to different matric decompositions. e.g. QR decomposition gives rotation. 
        
        see `symmetry_transforms.py`
    }
10/21 {
    dev note: 
        on hpc, archived old experiments (which are not sorted by time. Sorting by time is commit c55a369bc61f3fc1335ff992fa78276ffc2caee9) to experiments_old.tar.gz
        
    faster_sched_sampling_2022_Oct_20@15_28_39
        We need 30K teacher F. 
        Best disentangment seems to happen 5K epochs before validation loss minimum. 
    
    loss observations
        image pred loss ~ .0007 means predicting a blur
            (but sometimes balls with faithful but imprecise movements)
        linear proj MSE <0.2 means kinda good
    
    when repr augmentation DoF < z DoF, rnn can smuggle info past the augmentation. e.g. when you apply the SAME 2D translation to two balls in the same system (z has 4d), the augmentation is equivalent to one 2D transformation to 2d of the 4d z. 
}
10/22 {
    2022_m10_d22@01_27_00_tf_duration
        we need 40k sched samp. 
        See plots.pdf
        
        see evalDecoder
        height axis is sometimes not height. 
        when it's height, it's divided into two regions, one representing "unsure prediction" (blurry). The two regions are connected where the ball touches the ground, which is the only reasonable stitching plane in the z space, when your uncond prior is gaussian. 
    idea: z pred loss may need to stop grad from encoder. 
        I think this precents collapse. RNN output is pulled to encoded z. rnn input may be encouraged to collapse, but at least not directly. 
        z pred loss is desirable because image loss is pixel based and may not directly pull z in the right direction. 
        exp: stop_grad z_loss
    two-body dataset:
        focus on orbiting motions. 
        reject empty frame
        reject y-z plane only motion
        3D z (CM fixed) first
        fix wrong symm assumption! shared TUT instance
    trying incorrect symm augs. 
        Gus: just with SO(n) we have a lot to play with.
        exp: wrong_assum
    exp: vae_baseline
        research question: why can y axis be nice (linear)?
        Logically, VAE should be able to have nice linearity. 
        Let's see. 
        If still confused, next: 1d ball dataset. exclude disentanglement from the picture, and only test linearity. 
}
10/23 {
    2022_m10_d22@22_52_53_two_body
        self recon is good for both vae sizes. 
        Thoughts: Over-paramed VAE may be OKAY. 
            SPS should be able to regularize. 
            Big VAE shouldn't be nearly as harmful as big RNN. 
            Allowing big VAE. 
        Note to self:
            when doing two-body experiments, always remember to check evalDecoder! It's dangerous to think linearity metrics are enough - maybe two body solution is not linear in the first place. 
    2022_m10_d22@23_59_29_stop_grad
    2022_m10_d23@00_02_01_z_loss
        grad going into RNN also makes it collapse. It even make vae collapse. 
        Trying again with smaller weights. 
        If again collapses, try detaching the encoder from RNN input too. 
        if smaller weights work, 
            compare stop vs not stop
            also consider detaching encoder. 
        exp: z_loss
}
10/24 {
    2022_m10_d24@00_37_26_z_loss
        non-zero z loss leads to self-recon collapse. 
            Consider the gradient on the vae. We have already detached the encoder from ground truth when computing z loss. The only difference between zero z loss and non-zero z loss is with the gradient on the encoder as input to RNN. Therefore, JEPA also collapses the input encoder. next exp: detach. 
            more experiments: remove img pred loss? also detach encoder from input for img pred? 
            exp: z_loss
    2022_m10_d24@00_37_29_vae_baseline
        VAE (contra. AE) can indeed significantly encourage continuous z - coord mapping. Not disentangled, not linear, but continuous. Furthermore, it is not only continuous, but uniform-density. Interestingly, continuous + uniform density + (dim=1) => linearity. Maybe this is why we don't augment ball height but it performs well. 
        next exp: remove kld from symm training, see if height is messed up. 
        exp: no_kld
    2022_m10_d24@16_11_51_no_kld
        even without kld, ball height is linear! 
        Strange! I don't have a hypothesis, but next exp: supervise x and y and let AE learn z (i.e. ball height). 
        exp: supvise_xy

        observation: rnn_min_context = 4 does not contain initial bounce. this way, to know where the ground is, model has to accurately perceive the size of the ball. We should try increasing rnn_min_context. 
    2022_m10_d24@19_18_52_only_imagine
        this is to replicate Will's result with audio training where RNN never sees ground-truth z or image and is only trained to be symm-self-consistent. 
        There is no magical results.  
}
10/25 {
    2022_m10_d24@15_28_41_z_loss
        We have now detached both encoders. 
        It seems to work! less collapse. 
        Using evalDecoder shows: z loss is essential! weight=0 leads to poor results. 
        Question: does this kind of z loss help? 
        Question: Could the optimal weight be > 0.01?
        Next: A larger exp. 
    2022_m10_d25@00_24_32_min_context
        the most onconclusive results one can expect...
        Larger exp. 
    2022_m10_d24@23_57_39_two_body_nozl
        Look at the pred videos of rnn_width=48_rand_1. 
        Many predicitons look convincing to the eyes, but incorrect. They fail to get the correct distance between the balls and the camera. Currently the model can only use ball size. That's hard. What to do, Gus? 

        check evalDecoder. 
        model largely assumes stationary CM. 

        observation:
        why does it always use the first three dims? coincidence? 

        next: 1, use 3-d Z. 2, use non-orbit dataset. 
    2022_m10_d25@00_21_32_supvise_xy
        AE, with x and y supervised, can learn perfect z (i.e. ball height). 
        This is why we don't need to augment z! 
}
10/26 {
    2022_m10_d25@20_14_47_two_body_nozl
        check evalDecoder! 
            They like to use rotation as coords. 
            Smaller RNN seems better. (insignificant)
}
10/27 {
    2022_m10_d25@13_58_55_min_context
        rnn_min_context = 4 is pretty much GOOD ENOUGH. 
    2022_m10_d25@14_07_02_z_loss
        虽然我相信 z loss 大有益处， 但实验结果表明没有任何区别。
        干掉。
    2022_m10_d27@13_42_56_leave_view
        解耦不是很成功。
        out-of-frame z 也是 nonsense.
        see rand_1, which 解耦稍好， 但 out-of-frame z is nonsense.
    2022_m10_d27@13_42_53_rnn_width
        怎么现在解耦成功几率这么低...
        results are inconclusive. 
        Wait after dataset experiment, and do a bigger one. 
}
10/28 {
    2022_m10_d27@13_42_53_wrong_assum
        wrong symm assum makes it fail, even SO3...
        evalDecoder. 
    2022_m10_d27@15_30_51_energy
        Forgot to turn of prediction loss !?!? redo. 
        Also, the energy loss is near 0, but disentanglement is bad. 
            fake data too fake? GAN? 
            RNN is too storng? 
    2022_m10_d27@13_22_30_z_loss_nostop
        z loss is unecessary. 
    2022_m10_d27@18_20_46_grad_clip
        grad did not explode. We should use clip=1
}
10/29 {
    2022_m10_d27@13_56_47_dataset_size
        maybe even dataset size = 64 works! 
        For any conclusion, we do need fig 8. 
    2022_m10_d28@13_21_17_vae_two_body_norbit
        overfit very fast, and loss minima have bad recon. 
        I'm not sure what to do. Trying bigger VAE. 
    2022_m10_d28@13_46_57_energy
        energy loss is low, RL is bad. 
        Damn! Intuitively, such low-dim (3D) z space should be easy to fill with noise. 
        Comapred with diffusion tho, maybe higher dim is actually better. 
        next: try adding noise to data. 
    2022_m10_d28@18_49_52_tf_time
        evalEncoder: we may still need 40K even when grad_clip is 1 >> .03. 
        still, let's do another exp. 
}
10/31 {
    2022_m10_d29@14_00_43_tf_time
        evalEncoder
        解耦成功率咋这么低？
        我只能说, tf_time=40K seems optimal. 
}
11/2 {
    Let's try dropout. 
        another way to limit expressivity of RNN. 
        I apply it to the hidden state input only, NOT the output, NOT the z. 
        Next exp: vary dropout. 
        if dropout kills network performance, 
            Next exp: dropout=0.5, increase RNN size. 
}
11/3 {
    exp: self_recon
        just to see if disabling self_recon loss improves. 
        and also kinda test my exp framework. 
    6D twobody dataset: 
        currently, CM is random and moves. Wrong.
        make CM stationary but random across seqs.
    2022_m11_d04@00_27_53_self_recon:
        self recon loss helps the pred task generalize! 
            pred validation loss is smaller. 
    2022_m11_d04@00_27_53_rnn_width:
        all failed...
        Wait for successful disen, then try again. 
}
11/4 {
    2b data:
        seq len, DT -> 绕圈 >= 1
        traj z 纵深要有足够 variance
    我们现在强硬让两个东西一致
        - z 维数 = 对称性假设的维数
            如果打破： 就比如双体， 三维对称性加两次，加到 6d z
        - 真实对称性 和 对称性假设 一致
    exp: T(1) -> 0, 1; I -> 2
        TR(2) -> 0, 1; I -> 2, 3
}
11/5 {
    exp: sanity
        checking if we can have high success rate.
        if we do, maybe it's grad_clip / lr. 
    2022_m10_d28@13_46_57_energy
        energy loss ~= 1, which learned nothing. 
        Confused. maybe `z_transed` std >> 1? 
        next: trying to vary noise std. 
    2022_m11_d04@00_22_28_dropout
        is it collapsing? 
        trying smarter RNN + dropout. 
}
11/6 {
    rebuttal
        要提一下最初 shuffle 颜色的做法。
            对称性假设， 两种实现方法
                表征扩增
                锁死维度
                    是表征扩增方法针对训练的优化
        rnJU Weakness 2
            loudness: also T-invariant
                loudness-pitch SO(2)? 
        做实验： wrong assum
            read "surprising effectiveness"
                while the exp is running
            音乐：
                变 2D z, T(2), R(2)
            以下实验， 被操作维度为 z[:n], n \in {1, 2, 3}
            T(2)
            R(2)
            T(1), R(2)
            T(3), R(2)
            T(2), R(3)
}
11/15 {
    exp wrong assum for paper2022:
        eval_encoder: 
            就放这张图， 写 rebuttal + paper
            尽量先只动 appendix. A.2.4 写完了，看看正文里塞哪儿
            In analysis, add: 
                "we show that even with not-totally-correct symm assumptions, results are still good. See App. ..."
        shared K=0
        use boxplot
        see if we need 5 subfigs. prolly not
}
11/18 {
    2022_m11_d04@00_27_53_k
        nope. k doesn't matter. 
    rebuttal:
        go over all figure captions in appendix. explain
}
11/22 {
    2022_m11_d05@12_35_16_insanity
        indeed, disen prob ~= 15%
    2022_m11_d05@12_56_52_dropout
        seems: no dropout is best. However, try again after high success disen. 
    doing exp: exp_tr
        Trying Gus's method, doing T and R seperately. 
        motivation: My method still cannot match xj. What's different? TR? 
}
11/23 {
    2022_m11_d22@13_49_41_tr
        Gus method doesn't look good. 
        interesting (but irrevalent (prolly)) observation: Gus method has significantly lower image pred loss in the first half of training. 
    next, trying xj's dataset. 
}
11/24 {
    exp: xj_data
        This will be compared with 2022_m11_d05@12_35_16_insanity
}
11/26 {
    calc equivalent z loss weight as xuanjie's training
        0.0019
    2b data: DONE
        seq len, DT -> 绕圈 >= 1
        traj z 纵深要有足够 variance
        Gus: dataset looks good. Use! 
}
11/28 {
    next exp: self recon loss weight
        observe that with high K, the model collapses on x. 
        image pred is too hard, and it wants to collapse. 
        self recon is easier. since my models frequently collapse even when k=1, let's try emphasizing self recon. 
        exp_self_recon_weight
    2022_m11_d24@12_57_26_xj_data
        does not solve the problem yet. 
}
11/29 {
    2022_m11_d28@21_15_28_full_xj
        failed to replicate semantic equivalence
        damn, grad clip was on for xj. 
}
11/30 {
    2022_m11_d29@14_30_18_self_recon_weight
        higher recon weight cannot even stop collapse. 
        linear proj seems worse. 
        no use. 
    2022_m11_d29@14_34_57_full_xj
        xuanjie NB! 
        next, ablate. 
}
12/1 {
    2022_m11_d30@14_15_26_ablate_xj
        xj is right, that two params are important. 
}
12/2 meeting {
    2022_m11_d29@14_34_57_full_xj
        eval decoder. control z_3. 
        dan: bounce. xj: 碎了一地
        diff hyper param leads to two types of behaviors. Interesting. 
}
12/4 {
    2022_m12_d01@15_03_00_ablate_xj
        GusMethod seems superior! 
        Interesting! next exp: n_rand_init = 16. 
        Note GusMethod did not show advantage when using Dan's prev method. 
            see 2022_m11_d22@13_49_41_tr
}
12/5 {
    2022_m12_d04@16_23_02_ablate_xj
        confirms. GusMethod is better. 
        Why? idk. Ask Gus! 
}
12/6 {
    2022_m12_d06@17_33_43_ablate_xj
        not sure what's going on. trying BCE. 
}
12/7 {
    2022_m12_d07@03_45_15_ablate_xj
        BCE is helpful to prevent collapse. 
        probably achievable via MSE + weight tweaking too. 
        However, disen performance still xj > ablate. 
}
12/8 {
    2022_m12_d07@14_50_06_ablate_xj_residual
        omg! identity+noise initialization of rnn makes z collapse! 
    2022_m12_d07@14_50_06_ablate_xj_rnn_width
        rnn width 256 is better than 16
        Counter-intuitive! Why? 
        Try 32 next. 
    2022_m12_d07@15_15_49_ablate_xj_mse
        because we changed both rnn_width (which i thought wouldn't matter) and bce, this is inconclusive. 
}
12/9 {
    2022_m12_d08@14_16_32_ablate_xj_mse
        #present_in_meeting
        see /main/misc/bce_vs_mse.py
        so MSE is good to go! 
            this way, loss plots are easier to interprete.
            disen performance also increased. 
}
12/10 {
    2022_m12_d09@16_36_56_xj_rnn_width
        #present_in_meeting
        rnn_width=32 is much better than 256. 
        This makes sense. 
        However, proving that our method works well with 256 is important, because IRL ppl  overparametrize. 
    2022_m12_d09@16_36_56_xj_loss_w_refactor
        nan. 
        it turns out, Adam is not sensitive to total loss scaling. lr should not scale by 1e+6. Retry. 
}
12/11 {
    2022_m12_d10@14_42_11_xj_loss_w_refactor
        #present_in_meeting
        scaled weights look best. 
            maybe because of floatpoint errors. 
    2022_m12_d10@14_44_28_xj_k
        #present_in_meeting
        k=1 is better. 
}
12/12 {
    2022_m12_d11@16_30_27_xj_batch_size
        batch_size matters a lot. 128 is too large. 
        This is weird. next exp: more groups. Then, ask during meeting. 
    2022_m12_d11@16_30_27_xj_lr_dim
        seems there's more to find here. 
        next exp: more lr. 
}
12/13 {
    2022_m12_d12@14_31_16_xj_batch_size
        at first glance, smaller batch_size is better. 
        oh no, it's correlated with sched samp since XJ's sched samp looks at batch_i. 
    2022_m12_d12@14_31_16_xj_sched_samp
        #present_in_meeting
        No significant diff. using linear sched samp. 
    lr scheduling
        further studying xj code reveals the LR sched actually does not kick in because of a bug. 
}
12/14 {
    2022_m12_d13@16_26_35_xj_batch_size
        #present_in_meeting
        smaller batch size IS better. 
        Why? idk. 
        16 looks better than 32, but let's use 32 because it's faster. 
    2022_m12_d13@20_27_22_xj_neg_residual
        #present_in_meeting
        init RNN around negative identity is much worse than near-0 init. 
        at least we tried...
        thinking about it, initing nonzero kinda makes RNN want to collapse the z. 
            VAE would rather collapse for RNN than fixing RNN's prejudice. 
    Cool! xj ablation exps are done. integrate into main training templates. 
    2022_m12_d14@14_58_07_deep_spread
        #present_in_meeting
        deep_spread is good. It's staying
    2022_m12_d14@14_58_06_max_epoch
        differences are insignificant. one more exp, comparing 4000 vs 9000
    evalDecoder now controls the z-score of z instead of z itself
    integrate GusMethod into symm system. 
        now we have SAMPLE_TRANS | COMPOSE_TRANS
}
12/16 {
    2022_m12_d15@13_57_10_z_loss
        there are spikes in linear porj mse. 
            but not in training/validation losses. 
            Why? probably z collapsed and lin proj gave unstable results. 
        it looks like whether a run succeeds can be foretold at epoch 2000. 
            The linear porj mse curves rarely cross over. 
            self-recon also tells. 
                (there are many lines of self-recon loss ~= 0.001 on top of one another)
                predicting lin proj loss from self recon has 100% F1. 
            bascially, you cannot recover from z collapse. 
        z in (3.84e-3, 0.013) looks like a good place to test simsiam. 
    BTW 我才发现 QR decom 方法 sample 的随机旋转是允许翻折的。
        i.e. it is O(2) instead of SO(2)
    tell Gus about NYU HPC
    2022_m12_d15@15_38_17_sanity_check_howCombine
        utterly strange. the SAMPLE_TRANS symm seems different from GusMethod. 
        Debugging yields no results. 
        I can almost prove semantic equivalence. 
        I have no choice but to believe the exp results lied to the eyes. Redo. 
}
12/18 {
    2022_m12_d18@01_37_34_max_epoch
        9000 epochs is better than 4000.
}
todo {
    控制 max_batch vary batch_size
        如果还是 小 batch 好， 那就很能说明问题。
    test whether identity aug helps
    2D exp
        does symm allow over-param VAE? 
        vary 2d hParam
            - vae size
            - symm vs w/o symm

    test K=4 vs max_epoch *= 4
        this is related to optim stepping. 
        k=4: 
            each step is an avg of same traj multi aug
        max_epoch *= 4: 
            each step is an avg of multi traj multi aug

    can we early-stop via train-set self-recon or train-set linear proj mse? 
        see 12/16 notes
        observe other exps

    Try stop_grad w/ xj method, because SimSiam. 
    try remove z loss (after trying SimSiam!!!)
    try remove img pred loss and vary whether detach encoder

    test grad clip
    try bigger train set size
    test smaller dataset
    
    compare SAMPLE_TRANS vs COMPOSE_TRANS in two body. 
    try time aug, e.g. stretch (int / float ratio), reverse
    wrong symm: 11/4 notes
    vary K (aug ratio)
        make fig 8 in paper
            K = 0 1 4
        exp_k
        exp_data_size
    test kld weight = 0
    try RNN ensemble
    diffusion exp
        more cases. 4x4 matrix
        yihuajiemu half
}
archive {
    有些结， 升一维就能解开。 暂时升一维， 等会儿降回来？
}
if {
    if weights still Inf, add weight decay. 
}
